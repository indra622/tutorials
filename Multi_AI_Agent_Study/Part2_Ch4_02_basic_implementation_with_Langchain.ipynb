{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNbOnl/mfPSlNZtsbxbEOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indra622/tutorials/blob/master/Multi_AI_Agent_Study/Part2_Ch4_02_basic_implementation_with_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eube2ZIKc-eq",
        "outputId": "2b4bcebf-6d75-4815-9a09-3e270fd2513a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbTxFaUbdDHn",
        "outputId": "2eb65be9-3ad8-43af-c1f5-1ee627f797ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "client.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": \"2002년 월드컵 4강 국가 알려줘\"\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "M8LvQ9kHdIZN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43535df6-bd13-4a47-dfa8-9b11c81b27ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-AuqJF7YKQ1DMjnSgCP6Idq2rdDWTI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='2002년 FIFA 월드컵 4강에 진출한 국가는 한국, 독일, 브라질, 그리스입니다. 한국과 독일, 브라질과 그리스가 준결승에 진출하여 경기를 치뤘습니다. 결국 한국은 4강에서 독일에 패배하고, 브라질은 그리스에 승리하여 결승에 진출했습니다.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738110809, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=134, prompt_tokens=31, total_tokens=165, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMJjvDOPedJ6",
        "outputId": "68b0a692-8076-4f9f-a742-9e4a084022ba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.31)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.59.9)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (0.3.1)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.31->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.31->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.31->langchain-openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.31->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.31->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.31->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.31->langchain-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.2-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.2 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "chat = ChatOpenAI(\n",
        "    model_name = 'gpt-3.5-turbo',\n",
        ")\n",
        "\n",
        "chat.invoke(\"안녕 너를 소개해줄래?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqWKJxSne2Rx",
        "outputId": "1c7bad4b-e846-4776-cf5a-ccefe46a932f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 제 이름은 AI 어시스턴트 입니다. 제가 여러분들의 질문에 대답해드리고 도와드릴 수 있어요. 무엇을 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 22, 'total_tokens': 86, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a3d353d1-1251-4033-b2b6-64cfa37b5647-0', usage_metadata={'input_tokens': 22, 'output_tokens': 64, 'total_tokens': 86, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = (\n",
        "    PromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 {개수}추천하고,\n",
        "        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\n",
        "        <재료>\n",
        "        {재료}\n",
        "        \"\"\"\n",
        "    )\n",
        ")\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1DXco0be5fO",
        "outputId": "a3af1d0c-2ba0-4e04-e5bb-197caf047f45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['개수', '재료'], input_types={}, partial_variables={}, template='\\n        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 {개수}추천하고,\\n        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\\n        <재료>\\n        {재료}\\n        ')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.invoke({\"개수\":6, \"재료\":\"사과, 잼\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi6CQkjMfdO1",
        "outputId": "bcb84b55-68e0-46a7-a44b-8af824b5d5ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='\\n        너는 요리사야. 내가 가진 재료들을 갖고 만들 수 있는 요리를 6추천하고,\\n        그 요리의 레시피를 제시해줘. 내가 가진 재료는 아래와 같아.\\n        <재료>\\n        사과, 잼\\n        ')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # role, name 부여\n",
        "        (\"system\", \" You are a helpful AI bot. Your name is {name}.\"),\n",
        "        # 서로 안부를 묻고 답하는 히스토리를 미리 주입(few-shot)\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"i'm doing well, thanks!\"),\n",
        "        #실제 사용자 입력 전달\n",
        "        (\"human\", \"{user_input}\"),\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")\n",
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5uEJmVlfiGc",
        "outputId": "0f21b054-05f1-4823-c349-a6b1dd9e6c43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=' You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"i'm doing well, thanks!\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"topic\": \"bear\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9ZbWN6COgLAS",
        "outputId": "b9fae5ba-be93-4666-f2b4-9cb32fc0a374"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why do bears have hairy coats? \\n\\nBecause they look silly in sweaters!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "for s in chain.stream({\"topic\":\"bears\"}):\n",
        "  print(s.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAqgu9d4guj4",
        "outputId": "dbe35217-5d4c-4bec-9390-12cb654a3a78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do bears have sticky paws? \n",
            "\n",
            "Because they always bear-hug the honey!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                '너는 영화 전문가 AI야. 사용자가 원하는 장르의 영화를 리스트 형태로 추천해줘.'\n",
        "                'ex) Query: SF영화 3개 추천해줘 / 답변: [\"인터스텔라\", \"스페이스오디세이\", \"혹성탈출\"]'\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\")\n",
        "    ]\n",
        ")\n",
        "model = ChatOpenAI(model = 'gpt-4o-mini')\n",
        "chain = chat_template | model\n",
        "chain.invoke(\"액션\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpaVf10QhIZH",
        "outputId": "3ea499a5-ef2f-47e2-f5d2-056b8fd35ef5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='[\"다크 나이트\", \"존 윅\", \"매드맥스: 분노의 도로\"]', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 72, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-7623416a-69c2-488b-aef4-e46fb092e482-0', usage_metadata={'input_tokens': 72, 'output_tokens': 24, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "format_instructions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QqtLjdUripXk",
        "outputId": "7a546932-81a5-4662-9588-0b7b88083c41"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"List {subject}. answer in Korean \\n {format_instructions}\",\n",
        "    input_variables = [\"subject\"],\n",
        "    partial_variables = {\"format_instructions\": format_instructions},\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4o-mini')\n",
        "\n",
        "chain = prompt | model | output_parser\n",
        "chain.invoke({'subject': \"공포 영화\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0tBB1mzjb0A",
        "outputId": "cfe0d94e-59be-4a20-ec70-00ac65a39046"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['더 링',\n",
              " '미스트',\n",
              " '사탄의 인형',\n",
              " '괴물',\n",
              " '귀신이 산다',\n",
              " '인시디어스',\n",
              " '툼 레이더',\n",
              " '커뮤니티',\n",
              " '엑소시스트',\n",
              " '컨저링']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class Country(BaseModel):\n",
        "  continent: str = Field(description=\"사용자가 물어본 나라가 속한 대륙\")\n",
        "  population: str = Field(description =\"사용자가 물어본 나라의 인구(int 형식)\")\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Country)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query. \\n {format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "chain.invoke(\"아르헨티나는 어떤 나라야?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy9LIYSQj5aF",
        "outputId": "525087e9-d87d-4181-c400-94415ca51d2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'continent': 'South America', 'population': '45 million'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "RunnablePassthrough().invoke(\"안녕하세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D1lvy0ack6TD",
        "outputId": "5492036d-2883-4a81-cc6a-08a0ecf59fc5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"다음 한글 문장을 프랑스어로 번역해줘 {sentence}\n",
        "French sentence: (print from here)\"\"\")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "runnable_chain = {\"sentence\": RunnablePassthrough()} | prompt | model | StrOutputParser()\n",
        "\n",
        "runnable_chain.invoke(\"안녕하세요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KLAl0ruF8JCK",
        "outputId": "15513b28-f835-4814-afd7-578e484db4e6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요는 프랑스어로 \"Bonjour\"입니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(RunnablePassthrough.assign(mult=lambda x: x[\"num\"]*3)).invoke({\"num\":3})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgC585Gu81xL",
        "outputId": "433e8f3f-0cf1-4be3-fc36-381c00905202"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num': 3, 'mult': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "runnable = RunnableParallel(\n",
        "    extra=RunnablePassthrough.assign(mult=lambda x: x['num']*3),\n",
        "    modified=lambda x: x['num'] + 1,\n",
        ")\n",
        "\n",
        "runnable.invoke({\"num\":1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFGuZFf19Mvu",
        "outputId": "57af8ee3-7b50-436d-a27c-e9438e05678b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'extra': {'num': 1, 'mult': 3}, 'modified': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_smile(x):\n",
        "  return x+ \":)\""
      ],
      "metadata": {
        "id": "yigRGw2w9iXR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "runnable = RunnableLambda(add_smile)\n"
      ],
      "metadata": {
        "id": "_oJYkeAd9s76"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_str = \"{topic}의 역사에 대해 세문장으로 설명해 주세요\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "chain = prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "B1_HifO19wSn"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | StrOutputParser() | runnable\n",
        "\n",
        "chain.invoke(\"반도체\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "aN1hl_Z2-BbI",
        "outputId": "34bcfb6a-67d9-43ea-f3ef-552b749b2936"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'반도체의 역사는 19세기 말에 반도체 소재가 발견되면서 시작되었으며, 초기에는 주로 연구용으로 사용되었다. 20세기 중반, 트랜지스터의 발명으로 반도체 기술이 급격히 발전하며 컴퓨터 및 전자기기의 핵심 부품으로 자리잡았다. 이후 집적회로(IC)와 마이크로프로세서의 개발로 반도체 산업은 폭발적인 성장을 이루어 현재의 디지털 시대를 가능하게 했다.:)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = RunnableParallel(\n",
        "    passed = RunnablePassthrough(),\n",
        "    modified = lambda x: x['num'] + 1,\n",
        ")\n",
        "\n",
        "runnable.invoke({'num':1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jphKGiOe-SiU",
        "outputId": "efffcb50-6c3a-4d28-be7c-48e3745b366a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passed': {'num': 1}, 'modified': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable = RunnableParallel(\n",
        "    passed=RunnablePassthrough(),\n",
        "    modified = add_smile,\n",
        ")\n",
        "\n",
        "runnable.invoke('안녕하세요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rm1yYYj-tm6",
        "outputId": "9138eafb-e054-4685-ea95-541775f31268"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'passed': '안녕하세요', 'modified': '안녕하세요:)'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model='gpt-4o-mini', max_tokens=128, temperature=0)\n",
        "\n",
        "history_prompt = ChatPromptTemplate.from_template(\"{topic}이 무엇의 약자인지 알려주세요\")\n",
        "celeb_prompt = ChatPromptTemplate.from_template(\"{topic}분야의 유명인사 3명의 이름만 알려주세요\")\n",
        "\n",
        "history_chain = history_prompt | model | StrOutputParser()\n",
        "celeb_chain = celeb_prompt | model | StrOutputParser()\n",
        "\n",
        "map_chain = RunnableParallel(history=history_chain, celeb=celeb_chain)\n",
        "\n",
        "result = map_chain.invoke({\"topic\":\"영화\"})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuRFVn5U-5mM",
        "outputId": "15895fb1-dbb2-4402-848b-6fab234c3ea6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': '\"영화\"는 \"영상\"과 \"화면\"의 합성어로, 일반적으로 \"영화\"라는 용어는 특정한 약자가 아닙니다. 영화는 움직이는 이미지를 통해 이야기를 전달하는 예술 형식으로, 다양한 장르와 스타일이 존재합니다. \"영화\"라는 단어는 한국어에서 사용되는 일반적인 용어입니다.',\n",
              " 'celeb': '1. 스티븐 스필버그 (Steven Spielberg)\\n2. 마틴 스코세이지 (Martin Scorsese)\\n3. 앤젤리나 졸리 (Angelina Jolie)'}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBxIUhjd_kPu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}